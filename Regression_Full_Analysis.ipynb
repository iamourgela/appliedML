{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a770e0",
   "metadata": {},
   "source": [
    "# House Prices Regression Analysis\n",
    "This notebook follows a complete pipeline: EDA, preprocessing, modeling, and evaluation using linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Plot aesthetics\n",
    "sns.set_context(\"paper\", rc={\"font.size\":15, \"axes.titlesize\":15, \"axes.labelsize\":15})  \n",
    "plt.rcParams['axes.labelsize']  = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Regression models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Statsmodels (optional for OLS summary)\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, f1_score\n",
    "\n",
    "# Dimensionality reduction (optional)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Inline plotting for Jupyter\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72671e",
   "metadata": {},
   "source": [
    "## Step 2: Load and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a51f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Convert 'NA' strings in categorical columns to np.nan\n",
    "for df in [train, test]:\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].replace('NA', np.nan)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0397c23",
   "metadata": {},
   "source": [
    "## Step 3: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f889f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values using .info(), isnull().any() or any other way\n",
    "\n",
    "train.info()\n",
    "train.isnull().any()\n",
    "\n",
    "print(train.isnull().sum())\n",
    "\n",
    "test.info()\n",
    "test.isnull().any()\n",
    "\n",
    "print(test.isnull().sum())\n",
    "\n",
    "# Drop columns with >40% missing values\n",
    "drop_cols = train.columns[train.isnull().mean() > 0.4]\n",
    "train.drop(columns=drop_cols, inplace=True)\n",
    "test.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# Impute missing values\n",
    "num_cols = train.select_dtypes(include=['float64', 'int64']).drop(columns=['Id', 'SalePrice']).columns\n",
    "cat_cols = train.select_dtypes(include='object').columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "train[num_cols] = num_imputer.fit_transform(train[num_cols])\n",
    "test[num_cols] = num_imputer.transform(test[num_cols])\n",
    "train[cat_cols] = cat_imputer.fit_transform(train[cat_cols])\n",
    "test[cat_cols] = cat_imputer.transform(test[cat_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829aabb",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "corr = train.select_dtypes(include=['float64', 'int64']).corr()\n",
    "sns.heatmap(corr[['SalePrice']].sort_values(by='SalePrice', ascending=False), annot=True, cmap='coolwarm')\n",
    "plt.title('Feature Correlation with SalePrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c867f8d",
   "metadata": {},
   "source": [
    "## Step 5: Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b76560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "train_encoded = pd.get_dummies(train, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test, drop_first=True)\n",
    "\n",
    "# Align columns\n",
    "X = train_encoded.drop(columns=['SalePrice', 'Id'])\n",
    "y = train_encoded['SalePrice']\n",
    "X_test = test_encoded.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3465e2",
   "metadata": {},
   "source": [
    "## Step 6: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80eb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_scaled_df, y)\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "top_features = importances.sort_values(ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_features.values, y=top_features.index)\n",
    "plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e6d878",
   "metadata": {},
   "source": [
    "## Step 7: Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748722c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "from sklearn.pipeline import Pipeline\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.01),\n",
    "    'Polynomial Regression (deg 2)': Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('linreg', LinearRegression())])\n",
    "}\n",
    "\n",
    "# Use top features for poly regression\n",
    "X_poly = X_scaled_df[top_features.index]\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    if 'Polynomial' in name:\n",
    "        model.fit(X_poly, y)\n",
    "        y_pred = model.predict(X_poly)\n",
    "    else:\n",
    "        model.fit(X_scaled_df, y)\n",
    "        y_pred = model.predict(X_scaled_df)\n",
    "    \n",
    "    rmse = mean_squared_error(y, y_pred, squared=False)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    y_bin = pd.qcut(y, q=4, labels=False)\n",
    "    y_pred_bin = pd.qcut(pd.Series(y_pred).rank(method='first'), q=4, labels=False)\n",
    "    f1 = f1_score(y_bin, y_pred_bin, average='macro')\n",
    "    results.append({'Model': name, 'Train RMSE': round(rmse, 2), 'Train RÂ²': round(r2, 3), 'F1 Score': round(f1, 3)})\n",
    "\n",
    "pd.DataFrame(results).sort_values(by='Train RMSE')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
